# -*- coding: utf-8 -*-
"""User Identification From Walking Activity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IsjdB4TmwwgTFn-rJLUmKDQRMxHKe6g1
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use('seaborn-whitegrid')
sns.set_style("white")
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers import LSTM
from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import ConvLSTM2D
#from keras.utils import to_categorical
from keras import backend as K
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model, model_from_json
from keras.metrics import CategoricalAccuracy, CategoricalCrossentropy
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report

from numpy.random import seed
from tensorflow.random import set_seed

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,cross_validate

"""#Data Collection"""

!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00287/Activity%20Recognition%20from%20Single%20Chest-Mounted%20Accelerometer.zip"

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/Activity Recognition from Single Chest-Mounted Accelerometer.zip"

"""#Data Preparation"""

from glob import glob
files_path=glob(r"/content/Activity Recognition from Single Chest-Mounted Accelerometer/*.csv")

def Dataframe(files_path):
    df_participants=pd.DataFrame()
    for index,file_path in enumerate(files_path):
        df=pd.read_csv(file_path,header=None)
        df["User_id"]=int(re.sub(r'[^0-9]',"",file_path))
        df_participants=df_participants.append(df.iloc[:,1:])
    return df_participants

import re
data=Dataframe(files_path)
data.columns=['x_acceleration','y_acceleration','z_acceleration','Label','User_id']
data.index=range(len(data))

data=data.reindex(columns=["User_id","x_acceleration","y_acceleration","z_acceleration","Label"])

# removing outliers, values away from mean +- x*sd, from all classes for x, y and z
indexesToRemove = []
for x in data['Label'].unique().tolist():
    # filter with specific class
    print("Filtering label",x)
    df_temp = data[data['Label'] == x]

    for y in range(3):
        # remove rows with values away from mean by 3SD, in x, y and z
        mean = df_temp.iloc[:,y].mean()
        sd = df_temp.iloc[:,y].std()
        away = 3
        upperlimit = mean + away * sd
        lowerlimit = mean - away * sd

        i = df_temp[(df_temp.iloc[:,y] > upperlimit) | (df_temp.iloc[:,y] < lowerlimit)].index.tolist()
        indexesToRemove.extend(i)

indexesToRemove = list(set(indexesToRemove))
data.drop(indexesToRemove, axis=0, inplace=True)
data.reset_index(drop=True, inplace=True)
print("Removed",len(indexesToRemove),"rows.")

data

data.shape

# Counts of each participants
data.User_id.value_counts().sort_index()

# Checking Null values
data.isnull().sum()
## There are no Null values

# Checking some statistical values
data.describe()

# splitting by person recordings, first 12 for training and last 3 for test
X_train = data[data['User_id'] <= 12]
X_test = data[data['User_id'] > 12]
y_train = X_train.pop('Label')
y_test = X_test.pop('Label')
X_train.head()

# drop person column from both
X_train.drop('User_id', axis=1, inplace=True)
X_test.drop('User_id', axis=1, inplace=True)
X_train.head()

# generate training class weights which will be fed into model while training
from pprint import pprint
from sklearn.utils import class_weight
weights = class_weight.compute_class_weight('balanced', classes= np.unique(y_train), y=y_train)
train_class_weights = dict(zip(np.unique(y_train), weights))
pprint(train_class_weights)

import sklearn.preprocessing as skp
scaler = skp.MinMaxScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)

samples_per_sec = 52
window_seconds = 2
window_size = samples_per_sec * window_seconds

def sampleData(df, y):
    X_sampled = []
    y_sampled = []
    window = list(range(df.shape[0]))[::window_size]
    for x in range(len(window)-1):
        window_sample_X = df.iloc[window[x]:window[x+1],:].values.tolist()
        X_sampled.append(window_sample_X)

        window_sample_y = y.iloc[window[x]:window[x+1]].tolist()
        window_sample_y = max(window_sample_y, key=window_sample_y.count)
        y_sampled.append(window_sample_y)

    return np.array(X_sampled), np.array(y_sampled)

X_train_sampled, y_train_sampled = sampleData(X_train, y_train)
X_test_sampled, y_test_sampled = sampleData(X_test, y_test)

X_train_sampled.shape

"""#Model Building"""

import tensorflow as tf
from tensorflow import keras as k
ACCURACY_THRESHOLD = 0.95

class myCallback(k.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('val_sparse_categorical_accuracy') > ACCURACY_THRESHOLD):
            print("\n\nStopping training as we have reached our goal.")
            self.model.stop_training = True

acc_callback = myCallback()

def plotHistory(history):
    print("Max. Val. Accuracy -",max(history.history['val_sparse_categorical_accuracy']),"for epoch",str(int(np.argmax(history.history['val_sparse_categorical_accuracy'])+1)))
    pd.DataFrame(history.history).plot(figsize=(12,6))
    plt.show()

def setupCallbacks(bestModelPath):
    checkpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)
    LR = k.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, patience=5)
    cbsList = [checkpoint]
    # cbsList.append(LR)
    return cbsList

model = k.Sequential([


  k.layers.Bidirectional(k.layers.LSTM(512, return_sequences=True), input_shape=(window_size,3)),
  k.layers.Dropout(0.3),

  k.layers.Bidirectional(k.layers.LSTM(256, return_sequences=True)),
  k.layers.Dropout(0.3),

  k.layers.Bidirectional(k.layers.LSTM(128)),
  k.layers.Dropout(0.3),

  k.layers.Flatten(),
  k.layers.Dense(128, activation='relu'),
  k.layers.Dense(8),

])
model.summary()

tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)

# compiling with RMSProp & Adam to find best
bestModelPath = './best_model_2.hdf5'
callbacks_list = setupCallbacks(bestModelPath)
batch_size = 128
epochs = 20
model.compile(
    optimizer=k.optimizers.Adam(),
    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['sparse_categorical_accuracy']
)
history = model.fit(
                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled),
                    epochs=epochs, batch_size=batch_size,
                    class_weight=train_class_weights,
                    callbacks=[callbacks_list],
                    verbose=1
                   )

plotHistory(history)

model2 = k.Sequential([

    k.layers.Conv1D(filters=512, kernel_size=5, activation='relu', input_shape=(window_size,3)),

    k.layers.Bidirectional(k.layers.GRU(256, return_sequences=True)),
    k.layers.BatchNormalization(),
    k.layers.Dropout(0.3),

    k.layers.Bidirectional(k.layers.LSTM(128)),
    k.layers.Dropout(0.3),

    k.layers.Flatten(),
    k.layers.Dense(128, activation='relu'),
    k.layers.Dense(8),
])
model2.summary()

# compiling with RMSProp & Adam to find best
bestModelPath = './best_model_2.hdf5'
callbacks_list = setupCallbacks(bestModelPath)
batch_size = 128
epochs = 40
model2.compile(
    optimizer=k.optimizers.Adam(),
    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['sparse_categorical_accuracy']
)
history = model2.fit(
                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled),
                    epochs=epochs, batch_size=batch_size,
                    class_weight=train_class_weights,
                    callbacks=[callbacks_list],
                    verbose=1
                   )

plotHistory(history)

# compiling with RMSProp & Adam to find best
bestModelPath = './best_model_2.hdf5'
callbacks_list = setupCallbacks(bestModelPath)
batch_size = 128
epochs = 20
model2.compile(
    optimizer=k.optimizers.Adam(),
    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['sparse_categorical_accuracy']
)
history = model2.fit(
                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled),
                    epochs=epochs, batch_size=batch_size,
                    class_weight=train_class_weights,
                    callbacks=[callbacks_list],
                    verbose=1
                   )

tf.keras.utils.plot_model(
    model2,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)

plotHistory(history)

model3 = k.Sequential([

     k.layers.Conv1D(filters=512, kernel_size=5, activation='relu', input_shape=(window_size,3)),

     k.layers.Bidirectional(k.layers.LSTM(256, return_sequences=True)),
     k.layers.Dropout(0.2),

    k.layers.Bidirectional(k.layers.GRU(256)),
    k.layers.Dropout(0.2),

    k.layers.Flatten(),
    k.layers.Dense(128, activation='relu'),
    k.layers.Dense(8)

])
model3.summary()

# compiling with RMSProp & Adam to find best
bestModelPath = './best_model_2.hdf5'
callbacks_list = setupCallbacks(bestModelPath)
batch_size = 128
epochs = 20
model3.compile(
    optimizer=k.optimizers.Adam(),
    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['sparse_categorical_accuracy']
)
history = model3.fit(
                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled),
                    epochs=epochs, batch_size=batch_size,
                    class_weight=train_class_weights,
                    callbacks=[callbacks_list],
                    verbose=1
                   )

plotHistory(history)

"""#LSTM+GRU

"""

model4 = k.Sequential([

    k.layers.Conv1D(filters=512, kernel_size=5, activation='relu', input_shape=(window_size,3)),

    k.layers.Bidirectional(k.layers.LSTM(512, return_sequences=True)),
    k.layers.Dropout(0.3),

    k.layers.Bidirectional(k.layers.GRU(256)),
    k.layers.Dropout(0.3),


    k.layers.Flatten(),
    k.layers.Dense(128, activation='relu'),
    k.layers.Dense(8),
])
model4.summary()

# compiling with RMSProp & Adam to find best
bestModelPath = './best_model_2.hdf5'
callbacks_list = setupCallbacks(bestModelPath)
batch_size = 128
epochs = 20
model4.compile(
    optimizer=k.optimizers.Adam(),
    loss=k.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['sparse_categorical_accuracy']
)
history = model4.fit(
                    X_train_sampled, y_train_sampled, validation_data=(X_test_sampled, y_test_sampled),
                    epochs=epochs, batch_size=batch_size,
                    class_weight=train_class_weights,
                    callbacks=[callbacks_list],
                    verbose=1
                   )

plotHistory(history)